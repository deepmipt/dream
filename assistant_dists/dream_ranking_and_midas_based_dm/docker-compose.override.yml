services:
  agent:
    command: sh -c 'bin/wait && python -m deeppavlov_agent.run agent.pipeline_config=assistant_dists/dream_ranking_and_midas_based_dm/pipeline_conf.json'
    environment:
      WAIT_HOSTS: "sentseg:9011, ranking-and-intent-based-response-selector:9081,
          dff-intent-responder-skill:9012, intent-catcher:9014, ner:9021,
          factoid-qa:9071, kbqa:9072, entity-linking:9075, wiki-parser:9077, text-qa:9078,
          combined-classification:9087, fact-retrieval:9100, entity-detection:9103, dialogpt:9125,
          sentence-ranker:9128, property-extraction:9136, prompt-selector:9135, openai-api-chatgpt:9145, dff-dream-faq-prompted-skill:9170,
          dff-dream-persona-chatgpt-prompted-skill:9137, summarization-annotator:9058, dialog-summarizer:9059"
      WAIT_HOSTS_TIMEOUT: ${WAIT_TIMEOUT:-1000}
      HIGH_PRIORITY_INTENTS: 1
      RESTRICTION_FOR_SENSITIVE_CASE: 1
      ALWAYS_TURN_ON_ALL_SKILLS: 0
      LANGUAGE: EN
      FALLBACK_FILE: fallbacks_dream_en.json

  ranking-and-intent-based-response-selector:
    env_file: [ .env ]
    build:
      args:
        SERVICE_PORT: 9081
        SERVICE_NAME: response_selector
        LANGUAGE: EN
        SENTENCE_RANKER_ANNOTATION_NAME: sentence_ranker
        SENTENCE_RANKER_SERVICE_URL: http://sentence-ranker:9128/respond
        SENTENCE_RANKER_TIMEOUT: 3
        N_UTTERANCES_CONTEXT: 5
        FILTER_TOXIC_OR_BADLISTED: 1
        FALLBACK_FILE: fallbacks_dream_en.json
      context: .
      dockerfile: ./response_selectors/ranking_and_intent_based_response_selector/Dockerfile
    command: flask run -h 0.0.0.0 -p 9081
    environment:
      - FLASK_APP=server
    deploy:
      resources:
        limits:
          memory: 100M
        reservations:
          memory: 100M

  sentseg:
    env_file: [ .env ]
    build:
      context: ./annotators/SentSeg/
    command: flask run -h 0.0.0.0 -p 9011
    environment:
      - FLASK_APP=server
    deploy:
      resources:
        limits:
          memory: 1.5G
        reservations:
          memory: 1.5G

  dff-intent-responder-skill:
    env_file: [ .env ]
    build:
      args:
        SERVICE_PORT: 9012
        SERVICE_NAME: dff_intent_responder_skill
        INTENT_RESPONSE_PHRASES_FNAME: intent_response_phrases.json
      context: .
      dockerfile: ./skills/dff_intent_responder_skill/Dockerfile
    command: gunicorn --workers=1 server:app -b 0.0.0.0:9012 --reload
    deploy:
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 128M

  intent-catcher:
    env_file: [ .env ]
    build:
      context: .
      dockerfile: ./annotators/IntentCatcherTransformers/Dockerfile
      args:
        SERVICE_PORT: 9014
        CONFIG_NAME: intents_model_dp_config.json
        INTENT_PHRASES_PATH: intent_phrases.json
    command: /bin/bash -c 'CUDA_VISIBLE_DEVICES=$(nvidia-smi --query-gpu=memory.free --format=csv,nounits,noheader | nl -v 0 | sort -nrk 2 | cut -f 1 | head -n 1 | xargs) python -m flask run -h 0.0.0.0 -p 9014'
    environment:
      - FLASK_APP=server
    deploy:
      resources:
        limits:
          memory: 3.5G
        reservations:
          memory: 3.5G

  ner:
    env_file: [ .env ]
    build:
      args:
        CONFIG: ner_case_agnostic_multilingual_bert_base_extended.json
        SERVICE_PORT: 9021
        SRC_DIR: annotators/NER_deeppavlov
        COMMIT: f5117cd9ad1e64f6c2d970ecaa42fc09ccb23144
      context: ./
      dockerfile: annotators/NER_deeppavlov/Dockerfile
    command: /bin/bash -c 'CUDA_VISIBLE_DEVICES=$(nvidia-smi --query-gpu=memory.free --format=csv,nounits,noheader | nl -v 0 | sort -nrk 2 | cut -f 1 | head -n 1 | xargs) flask run -h 0.0.0.0 -p 9021'
    environment:
      - FLASK_APP=server
    tty: true
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 2G

  factoid-qa:
    env_file: [ .env ]
    build:
      args:
        SERVICE_PORT: 9071
        SERVICE_NAME: factoid_qa
      context: .
      dockerfile: ./skills/factoid_qa/Dockerfile
    command: flask run -h 0.0.0.0 -p 9071
    environment:
      - FLASK_APP=server
    deploy:
      resources:
        limits:
          memory: 256M
        reservations:
          memory: 256M

  entity-linking:
    env_file: [ .env ]
    build:
      args:
        SERVICE_PORT: 9075
        SERVICE_NAME: entity_linking
        CONFIG: entity_linking_eng.json
        SRC_DIR: annotators/entity_linking
      context: ./
      dockerfile: annotators/entity_linking/Dockerfile
    environment:
      - CUDA_VISIBLE_DEVICES=0
    deploy:
      resources:
        limits:
          memory: 2.5G
        reservations:
          memory: 2.5G

  wiki-parser:
    env_file: [ .env ]
    build:
      args:
        SERVICE_PORT: 9077
        SERVICE_NAME: wiki_parser
        WIKI_LITE_DB: http://files.deeppavlov.ai/kbqa/wikidata/wikidata2022.hdt
        WIKI_LITE_INDEX_DB: http://files.deeppavlov.ai/kbqa/wikidata/wikidata2022.hdt.index.v1-1
        WIKI_CACHE_DB: http://files.deeppavlov.ai/kbqa/wikidata/wikidata_cache.json
        CONFIG: wiki_parser.json
        SRC_DIR: annotators/wiki_parser
        COMMIT: ff5b156d16a949c3ec99da7fb60ae907dec37a41
        FAST: 1
      context: ./
      dockerfile: annotators/wiki_parser/Dockerfile
    command: /bin/bash -c 'CUDA_VISIBLE_DEVICES=$(nvidia-smi --query-gpu=memory.free --format=csv,nounits,noheader | nl -v 0 | sort -nrk 2 | cut -f 1 | head -n 1 | xargs) flask run -h 0.0.0.0 -p 9077'
    environment:
      - FLASK_APP=server
    deploy:
      resources:
        limits:
          memory: 256M
        reservations:
          memory: 256M

  text-qa:
    env_file: [ .env ]
    build:
      args:
        SERVICE_PORT: 9078
        SERVICE_NAME: text_qa
        CONFIG: qa_eng.json
      context: services/text_qa
      dockerfile: Dockerfile
    command: /bin/bash -c 'CUDA_VISIBLE_DEVICES=$(nvidia-smi --query-gpu=memory.free --format=csv,nounits,noheader | nl -v 0 | sort -nrk 2 | cut -f 1 | head -n 1 | xargs) flask run -h 0.0.0.0 -p 9078'
    environment:
      - FLASK_APP=server
      - LANGUAGE=EN
    deploy:
      resources:
        limits:
          memory: 3G
        reservations:
          memory: 3G


  dialogpt:
    env_file: [ .env ]
    build:
      args:
        SERVICE_PORT: 9125
        SERVICE_NAME: dialogpt
        PRETRAINED_MODEL_NAME_OR_PATH: microsoft/DialoGPT-medium
        N_HYPOTHESES_TO_GENERATE: 5
        CONFIG_NAME: dialogpt_en.json
        MAX_HISTORY_DEPTH: 2
      context: .
      dockerfile: ./services/dialogpt/Dockerfile
    command: /bin/bash -c 'CUDA_VISIBLE_DEVICES=$(nvidia-smi --query-gpu=memory.free --format=csv,nounits,noheader | nl -v 0 | sort -nrk 2 | cut -f 1 | head -n 1 | xargs) flask run -h 0.0.0.0 -p 9125'
    environment:
      - FLASK_APP=server
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 2G


  kbqa:
    env_file: [ .env ]
    build:
      args:
        SERVICE_PORT: 9072
        SERVICE_NAME: kbqa
        CONFIG: kbqa_cq_mt_bert_lite.json
        SRC_DIR: annotators/kbqa/
        COMMIT: 283a25e322e8fedc6ff0c159e4ec76bb165ae405
      context: ./
      dockerfile: annotators/kbqa/Dockerfile
    command: /bin/bash -c 'CUDA_VISIBLE_DEVICES=$(nvidia-smi --query-gpu=memory.free --format=csv,nounits,noheader | nl -v 0 | sort -nrk 2 | cut -f 1 | head -n 1 | xargs) flask run -h 0.0.0.0 -p 9072'
    environment:
      - FLASK_APP=server
    deploy:
      resources:
        limits:
          memory: 5G
        reservations:
          memory: 5G

  combined-classification:
    env_file: [ .env ]
    build:
      args:
        SERVICE_PORT: 9087
        SERVICE_NAME: combined_classification
        CONFIG: combined_classifier.json
      context: .
      dockerfile: ./annotators/combined_classification/Dockerfile
    command: /bin/bash -c 'CUDA_VISIBLE_DEVICES=$(nvidia-smi --query-gpu=memory.free --format=csv,nounits,noheader | nl -v 0 | sort -nrk 2 | cut -f 1 | head -n 1 | xargs) gunicorn --workers=1 server:app -b 0.0.0.0:9087 --timeout 600'
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 2G

  fact-retrieval:
    env_file: [ .env ]
    build:
      args:
        SERVICE_PORT: 9100
        SERVICE_NAME: fact_retrieval
        CONFIG: configs/fact_retrieval_page.json
        CONFIG_WIKI: configs/page_extractor.json
        CONFIG_WHOW: configs/whow_page_extractor.json
        SRC_DIR: annotators/fact_retrieval/
        COMMIT: 4b3e60c407644b750c9dc292ac6bf206081fb9d0
        N_FACTS: 3
      context: ./
      dockerfile: annotators/fact_retrieval/Dockerfile
    command: /bin/bash -c 'CUDA_VISIBLE_DEVICES=$(nvidia-smi --query-gpu=memory.free --format=csv,nounits,noheader | nl -v 0 | sort -nrk 2 | cut -f 1 | head -n 1 | xargs) flask run -h 0.0.0.0 -p 9100'
    environment:
      - FLASK_APP=server
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 4G

  entity-detection:
    env_file: [ .env ]
    build:
      args:
        SERVICE_NAME: entity_detection
        SEQ_TAG_CONFIG: wikipedia_entity_detection_distilbert.json
        CONFIG: entity_detection_eng.json
        LOWERCASE: 1
        SERVICE_PORT: 9103
        SRC_DIR: annotators/entity_detection/
        FINEGRAINED: 0
      context: ./
      dockerfile: annotators/entity_detection/Dockerfile
    command: /bin/bash -c 'CUDA_VISIBLE_DEVICES=$(nvidia-smi --query-gpu=memory.free --format=csv,nounits,noheader | nl -v 0 | sort -nrk 2 | cut -f 1 | head -n 1 | xargs) flask run -h 0.0.0.0 -p 9103'
    environment:
      - FLASK_APP=server
    deploy:
      resources:
        limits:
          memory: 2.5G
        reservations:
          memory: 2.5G

  prompt-selector:
    env_file: [ .env ]
    build:
      args:
        SERVICE_PORT: 9135
        SERVICE_NAME: prompt_selector
        SENTENCE_RANKER_SERVICE_URL: http://sentence-ranker:9128/respond
        N_SENTENCES_TO_RETURN: 3
        PROMPTS_TO_CONSIDER: dream_persona,dream_faq
      context: .
      dockerfile: ./annotators/prompt_selector/Dockerfile
    command: flask run -h 0.0.0.0 -p 9135
    environment:
      - FLASK_APP=server
    deploy:
      resources:
        limits:
          memory: 100M
        reservations:
          memory: 100M

  sentence-ranker:
    env_file: [ .env ]
    build:
      args:
        SERVICE_PORT: 9128
        SERVICE_NAME: sentence_ranker
        PRETRAINED_MODEL_NAME_OR_PATH: sentence-transformers/all-MiniLM-L6-v2
      context: ./services/sentence_ranker/
    command: /bin/bash -c 'CUDA_VISIBLE_DEVICES=$(nvidia-smi --query-gpu=memory.free --format=csv,nounits,noheader | nl -v 0 | sort -nrk 2 | cut -f 1 | head -n 1 | xargs) flask run -h 0.0.0.0 -p 9128'
    environment:
      - FLASK_APP=server
    deploy:
      resources:
        limits:
          memory: 3G
        reservations:
          memory: 3G

  openai-api-chatgpt:
    env_file: [ .env, .env_secret ]
    build:
      args:
        SERVICE_PORT: 9145
        SERVICE_NAME: openai_api_chatgpt
        PRETRAINED_MODEL_NAME_OR_PATH: gpt-3.5-turbo
        ENVVARS_TO_SEND: OPENAI_API_KEY,OPENAI_ORGANIZATION,OPENAI_API_BASE
      context: .
      dockerfile: ./services/openai_api_lm/Dockerfile
    command: /bin/bash -c 'CUDA_VISIBLE_DEVICES=$(nvidia-smi --query-gpu=memory.free --format=csv,nounits,noheader | nl -v 0 | sort -nrk 2 | cut -f 1 | head -n 1 | xargs) flask run -h 0.0.0.0 -p 9145'
    environment:
      - FLASK_APP=server
    deploy:
      resources:
        limits:
          memory: 500M
        reservations:
          memory: 100M


  dff-dream-persona-chatgpt-prompted-skill:
    env_file: [ .env,.env_secret ]
    build:
      args:
        SERVICE_PORT: 9137
        SERVICE_NAME: dff_dream_persona_prompted_skill
        PROMPT_FILE: common/prompts/dream_persona.json
        GENERATIVE_SERVICE_URL: http://openai-api-chatgpt:9145/respond
        GENERATIVE_SERVICE_CONFIG: openai-chatgpt.json
        GENERATIVE_TIMEOUT: 120
        N_UTTERANCES_CONTEXT: 7
        ENVVARS_TO_SEND: OPENAI_API_KEY,OPENAI_ORGANIZATION,OPENAI_API_BASE
      context: .
      dockerfile: ./skills/dff_template_prompted_skill/Dockerfile
    deploy:
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 128M


  property-extraction:
    env_file: [.env]
    build:
      args:
        CONFIG: t5_generative_ie_lite_infer.json
        SERVICE_PORT: 9136
        SRC_DIR: annotators/property_extraction/
        SERVICE_NAME: property_extraction
      context: ./
      dockerfile: annotators/property_extraction/Dockerfile
    command: flask run -h 0.0.0.0 -p 9136
    environment:
      - FLASK_APP=server
    deploy:
      resources:
        limits:
          memory: 7G
        reservations:
          memory: 7G

  dff-dream-faq-prompted-skill:
    env_file: [ .env,.env_secret ]
    build:
      args:
        SERVICE_PORT: 9170
        SERVICE_NAME: dff_dream_faq_prompted_skill
        PROMPT_FILE: common/prompts/dream_faq.json
        GENERATIVE_SERVICE_URL: http://openai-api-chatgpt:9145/respond
        GENERATIVE_SERVICE_CONFIG: openai-chatgpt.json
        GENERATIVE_TIMEOUT: 120
        N_UTTERANCES_CONTEXT: 7
        ENVVARS_TO_SEND: OPENAI_API_KEY,OPENAI_ORGANIZATION,OPENAI_API_BASE
      context: .
      dockerfile: ./skills/dff_template_prompted_skill/Dockerfile
    deploy:
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 128M


  summarization-annotator:
    env_file: [ .env ]
    build:
      args:
        SERVICE_PORT: 9058
        SERVICE_NAME: summarization_annotator
        SUMMARIZATION_REQUEST_TIMEOUT: 10
      context: ./annotators/summarization_annotator/
    command: flask run -h 0.0.0.0 -p 9058
    environment:
      - FLASK_APP=server
    deploy:
      resources:
        limits:
          memory: 256M
        reservations:
          memory: 256M

  dialog-summarizer:
    env_file: [ .env ]
    build:
      args:
        SERVICE_PORT: 9059
        SERVICE_NAME: dialog_summarizer
        PRETRAINED_MODEL_NAME: "knkarthick/MEETING_SUMMARY"
      context: ./services/dialog_summarizer/
    command: /bin/bash -c 'CUDA_VISIBLE_DEVICES=$(nvidia-smi --query-gpu=memory.free --format=csv,nounits,noheader | nl -v 0 | sort -nrk 2 | cut -f 1 | head -n 1 | xargs) flask run -h 0.0.0.0 -p 9059'
    environment:
      - FLASK_APP=server
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 4G

version: '3.7'
